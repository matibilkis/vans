{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import warnings\n",
    "from collections import deque\n",
    "import random\n",
    "from vans_gym.envs import VansEnv\n",
    "from vans_gym.solvers import PennylaneSolver\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self,tau=0.01, seed_val = 0.05):\n",
    "        super(Critic,self).__init__()\n",
    "\n",
    "        self.tau = tau\n",
    "        self.l1 = Dense(10,kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "        bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val))\n",
    "\n",
    "        self.l2 = Dense(8, kernel_initializer=tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val),\n",
    "                        bias_initializer = tf.random_uniform_initializer(minval=-seed_val, maxval=seed_val)) #n_actions in the alphabet\n",
    "\n",
    "\n",
    "\n",
    "    def update_target_parameters(self,primary_net):\n",
    "        prim_weights = primary_net.get_weights()\n",
    "        targ_weights = self.get_weights()\n",
    "        weights = []\n",
    "        for i in tf.range(len(prim_weights)):\n",
    "            weights.append(self.tau * prim_weights[i] + (1 - self.tau) * targ_weights[i])\n",
    "        self.set_weights(weights)\n",
    "        return\n",
    "\n",
    "    def give_action(self,state, ep=0.01, more_states=1):\n",
    "        if np.random.random() < ep:\n",
    "            random_action = np.random.choice(range(8))\n",
    "            return random_action\n",
    "        else:\n",
    "            qvals = np.squeeze(self(tf.expand_dims(state, axis=0)))\n",
    "            action_gredy = np.random.choice(np.where(qvals == np.max(qvals))[0])\n",
    "            return action_gredy\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        feat = tf.nn.sigmoid(self.l1(inputs))\n",
    "        feat = tf.nn.sigmoid(self.l2(feat))\n",
    "        return feat\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size=10**3):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def add(self, experience):\n",
    "        if not isinstance(experience, tuple):\n",
    "            raise ValueError(\"buffer wants tuples!\")\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, int(batch_size))\n",
    "        return batch\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_step(critic, buffer, optimizer, batch_size=250):\n",
    "    batch =buffer.sample(batch_size)\n",
    "    states, actions, next_states, rewards, dones = np.transpose(batch)\n",
    "\n",
    "    qpreds = critic(tf.stack(states))\n",
    "    labels = qpreds.numpy()\n",
    "    for inda, act in enumerate(actions):\n",
    "        if dones[inda] is False:\n",
    "            labels[inda,act] = critic.give_action(next_states[inda], ep=0)\n",
    "        else:\n",
    "            labels[inda, act] = rewards[inda] \n",
    "            \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(critic.trainable_variables)\n",
    "        qpreds = critic(tf.stack(states))\n",
    "\n",
    "        loss = tf.keras.losses.MSE(labels, qpreds)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        grads = tape.gradient(loss, critic.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 9836/9999 [19:17<00:25,  6.33it/s]"
     ]
    }
   ],
   "source": [
    "buffer = ReplayBuffer()\n",
    "critic = Critic()\n",
    "n_qubits = 3\n",
    "solver = PennylaneSolver(n_qubits, combinatorial_only=True)\n",
    "\n",
    "env = VansEnv(solver, 8, mdp_length=2, state_as_sequence=True, printing=False)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "\n",
    "r=[]\n",
    "pt=[]\n",
    "cumre=0\n",
    "episodes = np.arange(1,10**4,1)\n",
    "tt = .75*len(episodes)/np.log(1/0.05)\n",
    "def schedule(k):\n",
    "    if k< 10:\n",
    "        return 1\n",
    "    else:\n",
    "        return max(0.1, np.exp(-k/tt))\n",
    "    \n",
    "for k in tqdm(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = critic.give_action(state, ep=schedule(k))\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        buffer.add((state, action, next_state, reward, done))\n",
    "        state = next_state\n",
    "    cumre+=reward\n",
    "    r.append(cumre)\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = critic.give_action(state, ep=0)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state\n",
    "    if (k>10**3):\n",
    "        learning_step(critic, buffer, optimizer)\n",
    "    pt.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r/episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt[400:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
