{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i explore ep-greedy with DQN for the dataset I have created (3d with 2 qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vans_gym.envs import VansEnvsSeq\n",
    "from vans_gym.solvers import CirqSolverR, Checker\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "solver = CirqSolverR(n_qubits = 2, observable_name=\"Ising_\",qlr=0.05,qepochs=100)\n",
    "checker = Checker(solver)\n",
    "\n",
    "env = VansEnvsSeq(solver,checker=checker, depth_circuit=3)\n",
    "\n",
    "gates_number = len(solver.alphabet) - solver.n_qubits\n",
    "current=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import warnings\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm as tqdm\n",
    "import os\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self,tau=0.05, seed_val = 0.05, n_actions=6):\n",
    "        super(Critic,self).__init__()\n",
    "\n",
    "        self.tau = tau\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.l1 = Dense(60)\n",
    "        self.l2 = Dense(60)\n",
    "        self.l3 = Dense(n_actions)\n",
    "    \n",
    "\n",
    "\n",
    "    def update_target_parameters(self,primary_net):\n",
    "        prim_weights = primary_net.get_weights()\n",
    "        targ_weights = self.get_weights()\n",
    "        weights = []\n",
    "        for i in tf.range(len(prim_weights)):\n",
    "            weights.append(self.tau * prim_weights[i] + (1 - self.tau) * targ_weights[i])\n",
    "        self.set_weights(weights)\n",
    "        return\n",
    "    \n",
    "    @tf.function\n",
    "    def greedy_act(self, tf_state):\n",
    "        return tf.argmax(self(tf_state), axis=-1)\n",
    "\n",
    "    def give_action(self,state, ep=0.01):\n",
    "        if np.random.random() < ep:\n",
    "            random_action = np.random.choice(range(self.n_actions))\n",
    "            return random_action\n",
    "        else:\n",
    "            idx = self.greedy_act(tf.expand_dims(np.array(state), axis=0))\n",
    "            idx =idx.numpy()[0]\n",
    "            return idx\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feat = self.l1(inputs)\n",
    "        feat = tf.nn.relu(feat)\n",
    "        feat = self.l2(feat)\n",
    "        feat = tf.nn.relu(feat)\n",
    "        feat = self.l3(feat)\n",
    "        feat = tf.nn.sigmoid(feat)\n",
    "        return feat\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size=10**6):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        self.priorities = []\n",
    "        self.ps=.8\n",
    "        \n",
    "    def add(self, experience, priority=0):\n",
    "        if not isinstance(experience, tuple):\n",
    "            raise ValueError(\"buffer wants tuples!\")\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "            self.priorities.append(priority + 0.01) \n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "            self.priorities[self.count] = priority + 0.01 #the most recent is self.count\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = []\n",
    "        pro=(np.array(self.priorities)**self.ps)/np.sum(np.array(self.priorities)**self.ps)\n",
    "        if self.count < batch_size:\n",
    "            indices = np.random.choice(range(self.count), self.count, p=pro)\n",
    "        else:\n",
    "            indices = np.random.choice(range(self.count), int(batch_size), p=pro)\n",
    "        for idx in indices:\n",
    "            batch.append(self.buffer[idx])\n",
    "        return batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"data_testing_algo/energies_3d_2q.pickle\", \"rb\") as dictt:\n",
    "    energies = pickle.load(dictt)\n",
    "\n",
    "with open (\"data_testing_algo/next_states_3d_2q.pickle\", \"rb\") as dictt:\n",
    "    next_states = pickle.load(dictt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_experiences():\n",
    "    for k in tqdm(range(1000)):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        stuck_count=0\n",
    "        episode=[]\n",
    "        while not done:\n",
    "            action = net.give_action(state, ep=1)\n",
    "            next_state = next_states[str(np.array(state).astype(np.int64))][str(action)]\n",
    "            if len(np.where(next_state == -1)[0])==0:\n",
    "                done = True\n",
    "                reward = energies[str(np.array(next_state).astype(np.int64))]\n",
    "            else:\n",
    "                reward=0.\n",
    "                if stuck_count>5:\n",
    "                    done=True\n",
    "            episode.append((state, action, next_state, reward, done))\n",
    "            stuck_count+=1\n",
    "            state=next_state\n",
    "        for step in episode:\n",
    "            buffer.add(step, priority=reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(buffer, net, optimizer):\n",
    "    batch_size=32\n",
    "    batch =buffer.sample(batch_size)\n",
    "    states, actions, ns, rewards, dones = np.transpose(batch)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(net.trainable_variables)\n",
    "        qpreds = net(tf.stack(states))\n",
    "        Q_sP1_greedy = tf.math.reduce_max(net(tf.stack(ns)), axis=-1)\n",
    "        target_q = rewards + (1-dones)*Q_sP1_greedy\n",
    "\n",
    "        Q_update = tf.reduce_sum(tf.multiply(qpreds, tf.keras.utils.to_categorical(actions, net.n_actions)), axis=1)\n",
    "\n",
    "        loss = tf.reduce_mean(tf.keras.losses.MeanSquaredError()(target_q, Q_update))\n",
    "        grads = tape.gradient(loss, net.trainable_variables)\n",
    "        \n",
    "    #c=0\n",
    "    #for gra, var in zip(grads, net.trainable_variables):\n",
    "    #    grads[c] = tf.clip_by_value(grads[c], -0.1,0.1)\n",
    "    #c+=1\n",
    "    #grads = [tf.clip_by_value(k, np.min(), self.clip_rew) for k in tape.gradient(loss, self.prim_qnet.trainable_variables)]\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2725.56it/s]\n"
     ]
    }
   ],
   "source": [
    "buffer = ReplayBuffer()\n",
    "net = Critic()\n",
    "add_experiences()\n",
    "state = tf.random.uniform((1,1,3))\n",
    "net(state)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=10**-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<03:19,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer critic_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer critic_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "100%|██████████| 1000/1000 [00:13<00:00, 76.81it/s]\n"
     ]
    }
   ],
   "source": [
    "name=\"runs\"\n",
    "current+=1\n",
    "info=\"lr10**-2\"\n",
    "fw_loss = tf.summary.create_file_writer(name+\"/\"+str(current)+info)\n",
    "fw_greedy= tf.summary.create_file_writer(name+\"/\"+str(current)+info)\n",
    "for k in tqdm(range(10**3)):\n",
    "    l = train_step(buffer,net,optimizer)\n",
    "    with fw_loss.as_default():\n",
    "         tf.summary.scalar('loss', l, step=k)\n",
    "            \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    stuck_count=0\n",
    "    while not done:\n",
    "        action = net.give_action(state, ep=0)\n",
    "        next_state = next_states[str(np.array(state).astype(np.int64))][str(action)]\n",
    "        if len(np.where(next_state == -1)[0])==0:\n",
    "            done = True\n",
    "            reward = energies[str(np.array(next_state).astype(np.int64))]\n",
    "        else:\n",
    "            reward=0.\n",
    "            if stuck_count>3:\n",
    "                done=True\n",
    "        stuck_count+=1\n",
    "        state=next_state\n",
    "\n",
    "    with fw_greedy.as_default():\n",
    "        tf.summary.scalar('greedy energy', tf.convert_to_tensor(reward), step=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[0.99951386, 0.9948768 , 0.9503937 , 0.99988604, 0.05652143,\n",
       "        0.15273295]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(tf.expand_dims(np.array([5,0,-1]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.99999976, dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energies[\"[5 0 3]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[0 1 0]', 0.0),\n",
       " ('[0 1 2]', array(-4.0865985e-17, dtype=float32)),\n",
       " ('[0 3 1]', array(2.435396e-17, dtype=float32)),\n",
       " ('[0 4 1]', -4.2146848e-08),\n",
       " ('[0 1 5]', -4.2146848e-08),\n",
       " ('[2 0 1]', array(-8.458828e-18, dtype=float32)),\n",
       " ('[2 0 3]', array(1.4974907e-17, dtype=float32)),\n",
       " ('[2 0 4]', array(-3.9502556e-08, dtype=float32)),\n",
       " ('[2 5 0]', array(-6.474444e-08, dtype=float32)),\n",
       " ('[0 3 0]', array(-5.2230593e-17, dtype=float32)),\n",
       " ('[0 4 3]', array(-5.2390664e-08, dtype=float32)),\n",
       " ('[0 3 5]', array(-5.2217576e-08, dtype=float32)),\n",
       " ('[0 4 0]', 4.371138e-08),\n",
       " ('[0 4 2]', array(0.9999929, dtype=float32)),\n",
       " ('[0 4 4]', -8.429369e-08),\n",
       " ('[0 4 5]', -8.429368e-08),\n",
       " ('[5 0 1]', 4.371138e-08),\n",
       " ('[5 0 3]', array(0.99999976, dtype=float32)),\n",
       " ('[5 0 4]', -8.429368e-08),\n",
       " ('[5 5 0]', -8.429369e-08),\n",
       " ('[1 0 1]', 0.0),\n",
       " ('[1 2 0]', array(-5.411548e-17, dtype=float32)),\n",
       " ('[1 0 3]', array(-1.0179708e-16, dtype=float32)),\n",
       " ('[1 0 4]', -4.2146848e-08),\n",
       " ('[1 5 0]', -4.2146848e-08),\n",
       " ('[1 2 1]', array(-3.4318367e-18, dtype=float32)),\n",
       " ('[1 2 3]', array(2.950278e-17, dtype=float32)),\n",
       " ('[1 2 4]', array(-4.0826936e-08, dtype=float32)),\n",
       " ('[1 2 5]', array(-2.8375215e-08, dtype=float32)),\n",
       " ('[3 1 0]', array(2.5763632e-18, dtype=float32)),\n",
       " ('[3 1 2]', array(2.5718829e-17, dtype=float32)),\n",
       " ('[4 3 1]', array(-4.1712838e-08, dtype=float32)),\n",
       " ('[3 1 5]', array(-6.215793e-08, dtype=float32)),\n",
       " ('[4 1 0]', 4.371138e-08),\n",
       " ('[4 1 2]', array(0.99999386, dtype=float32)),\n",
       " ('[4 4 1]', -8.429369e-08),\n",
       " ('[4 1 5]', -8.429368e-08),\n",
       " ('[1 5 1]', 4.371138e-08),\n",
       " ('[1 5 3]', array(0.9999902, dtype=float32)),\n",
       " ('[1 5 5]', -8.429369e-08),\n",
       " ('[2 1 0]', array(5.8897245e-17, dtype=float32)),\n",
       " ('[2 1 2]', array(1.8498397e-18, dtype=float32)),\n",
       " ('[2 3 1]', array(-2.1845236e-18, dtype=float32)),\n",
       " ('[2 4 1]', array(-4.212177e-08, dtype=float32)),\n",
       " ('[2 1 5]', array(-4.222914e-08, dtype=float32)),\n",
       " ('[2 3 0]', array(-2.4825085e-16, dtype=float32)),\n",
       " ('[2 4 3]', array(-3.8938925e-08, dtype=float32)),\n",
       " ('[2 3 5]', array(-8.37977e-08, dtype=float32)),\n",
       " ('[2 4 0]', array(4.371138e-08, dtype=float32)),\n",
       " ('[2 4 2]', array(0.9999714, dtype=float32)),\n",
       " ('[2 4 4]', array(-8.377403e-08, dtype=float32)),\n",
       " ('[2 4 5]', array(-5.9291793e-08, dtype=float32)),\n",
       " ('[2 5 1]', array(4.371138e-08, dtype=float32)),\n",
       " ('[2 5 3]', array(0.99999845, dtype=float32)),\n",
       " ('[2 5 5]', array(-8.354825e-08, dtype=float32)),\n",
       " ('[3 0 1]', array(4.5504628e-17, dtype=float32)),\n",
       " ('[3 0 3]', array(1.6747202e-16, dtype=float32)),\n",
       " ('[3 0 4]', array(-4.2146702e-08, dtype=float32)),\n",
       " ('[3 5 0]', array(-4.345987e-08, dtype=float32)),\n",
       " ('[4 3 0]', array(4.371138e-08, dtype=float32)),\n",
       " ('[4 2 3]', array(0.9999833, dtype=float32)),\n",
       " ('[4 4 3]', array(-8.429369e-08, dtype=float32)),\n",
       " ('[4 3 5]', array(-8.4303664e-08, dtype=float32)),\n",
       " ('[3 5 1]', array(4.371138e-08, dtype=float32)),\n",
       " ('[3 5 3]', array(0.99997586, dtype=float32)),\n",
       " ('[3 5 5]', array(-8.558632e-08, dtype=float32)),\n",
       " ('[4 0 1]', -8.585823e-08),\n",
       " ('[4 2 0]', array(4.3697696e-08, dtype=float32)),\n",
       " ('[4 0 3]', array(6.204786e-08, dtype=float32)),\n",
       " ('[4 0 4]', 1.564541e-09),\n",
       " ('[4 5 0]', 1.7457797e-08),\n",
       " ('[4 2 1]', array(0.99998844, dtype=float32)),\n",
       " ('[4 2 4]', array(0.9999796, dtype=float32)),\n",
       " ('[4 2 5]', array(0.9999863, dtype=float32)),\n",
       " ('[4 4 0]', -9.032173e-15),\n",
       " ('[4 4 2]', array(6.997512e-08, dtype=float32)),\n",
       " ('[4 4 4]', -1.2644051e-07),\n",
       " ('[4 4 5]', -1.2644051e-07),\n",
       " ('[4 5 1]', 1.7457797e-08),\n",
       " ('[4 5 3]', array(0.9999894, dtype=float32)),\n",
       " ('[4 5 5]', -1.2644053e-07),\n",
       " ('[5 1 0]', -8.585823e-08),\n",
       " ('[5 1 2]', array(6.152069e-08, dtype=float32)),\n",
       " ('[5 3 1]', array(4.371136e-08, dtype=float32)),\n",
       " ('[5 1 5]', 1.564541e-09),\n",
       " ('[5 3 0]', array(0.9999846, dtype=float32)),\n",
       " ('[5 3 5]', array(0.99997526, dtype=float32)),\n",
       " ('[5 5 1]', -9.032173e-15),\n",
       " ('[5 5 3]', array(7.464211e-08, dtype=float32)),\n",
       " ('[5 5 5]', -1.2644051e-07)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(energies.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[1.0381008, 1.056144 , 1.0013628, 1.0380746, 1.0129645, 1.0406094]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(tf.expand_dims(np.array([5,-1,-1]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[1.        , 0.99998057, 0.9999523 , 0.99999154, 0.99999976,\n",
       "        0.99999964]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(tf.expand_dims(np.array([-1,-1,-1]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[1.        , 0.99965024, 0.99985254, 0.9999442 , 0.9999994 ,\n",
       "        0.9999975 ]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(tf.expand_dims(np.array([0,-1,-1]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[0.7090682 , 0.9918829 , 0.31462577, 0.05386559, 0.01243235,\n",
       "        0.00883244]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(tf.expand_dims(np.array([0,0,1]), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So the problem is that this fucking sequential thing collapses, because relates everything with everything... any smart way to put some structure that is not a RNN ? Let's jump to RNN !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[0 0 1]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-51bf90097b10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menergies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"[0 0 1]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '[0 0 1]'"
     ]
    }
   ],
   "source": [
    "energies[\"[0 0 1]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = buffer.sample(buffer.count)\n",
    "states, actions, ns, rewards, dones = np.transpose(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.array(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_step(critic, buffer, optimizer, batch_size=30):\n",
    "    batch =buffer.sample(batch_size)\n",
    "    states, actions, next_states, rewards, dones = np.transpose(batch)\n",
    "\n",
    "    qpreds = critic(tf.stack(states))\n",
    "    labels = qpreds.numpy()\n",
    "    for inda, act in enumerate(actions):\n",
    "        if dones[inda] is False:\n",
    "            labels[inda,act] = np.max(np.squeeze(critic_target(tf.expand_dims(next_states[inda], axis=0))))\n",
    "        else:\n",
    "            labels[inda, act] = rewards[inda]\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(critic.trainable_variables)\n",
    "        qpreds = critic(tf.stack(states))\n",
    "\n",
    "        loss = tf.keras.losses.MSE(labels, qpreds)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        grads = tape.gradient(loss, critic.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, critic.trainable_variables))\n",
    "    critic_target.update_target_parameters(critic)\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Critic()\n",
    "state = tf.random.uniform((1,1,3))\n",
    "net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from vans_gym.envs import VansEnvsSeq\n",
    "from vans_gym.solvers import CirqSolverR, Checker\n",
    "from vans_gym.models import DQN\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "solver = CirqSolverR(n_qubits = 2, observable_name=\"Ising_\",qlr=0.05,qepochs=100)\n",
    "checker = Checker(solver)\n",
    "\n",
    "env = VansEnvsSeq(solver,checker=checker, depth_circuit=3)\n",
    "\n",
    "gates_number = len(solver.alphabet) - solver.n_qubits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = DQN(env,name=\"wed\",use_per=False, learning_rate=0.1, tau=1, ep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tqdm(range(1000)):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    stuck_count=0\n",
    "    while not done:\n",
    "        action = Model.give_action(state, ep=1)\n",
    "        next_state = next_states[str(np.array(state).astype(np.int64))][str(action)]\n",
    "        if len(np.where(next_state == -1)[0])==0:\n",
    "            done = True\n",
    "            reward = energies[str(np.array(next_state).astype(np.int64))]\n",
    "        else:\n",
    "            reward=0.\n",
    "            if stuck_count>10:\n",
    "                done=True\n",
    "        Model.replay_buffer.add_experience(action, [state, next_state], reward, done)\n",
    "        stuck_count+=1\n",
    "        state=next_state\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tqdm(range(2000)):\n",
    "    with Model.fw_loss.as_default():\n",
    "        tf.summary.scalar('loss', Model.learn_step(batch_size=32), step=k)\n",
    "        \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    stuck_count=0\n",
    "    while not done:\n",
    "        action = Model.give_action(state, ep=0)\n",
    "        next_state = next_states[str(np.array(state).astype(np.int64))][str(action)]\n",
    "        if len(np.where(next_state == -1)[0])==0:\n",
    "            done = True\n",
    "            reward = energies[str(np.array(next_state).astype(np.int64))]\n",
    "        else:\n",
    "            reward=0.\n",
    "            if stuck_count>5:\n",
    "                done=True\n",
    "        stuck_count+=1\n",
    "        state=next_state\n",
    "\n",
    "    with Model.fw_greedy.as_default():\n",
    "        tf.summary.scalar('greedy energy', tf.convert_to_tensor(reward), step=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
